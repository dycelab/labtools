{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92af5987-c1e4-4d0e-8774-9d9fe091c5ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import requests\n",
    "from getpass import getpass\n",
    "import sys\n",
    "import time\n",
    "import re\n",
    "import threading\n",
    "import datetime\n",
    "import os\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import geojson\n",
    "import pprint\n",
    "from geojson import Polygon, Feature, FeatureCollection, dump\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb80a032-5915-47d2-baa4-d5190adcb7c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# user inputs\n",
    "################################################\n",
    "# change tile number\n",
    "tile_number = 'h3v9'\n",
    "# change data set name\n",
    "datasetName = 'landsat_ard_tile_c2'\n",
    "#  sensor list comment uncomment as needed\n",
    "sensors = [\n",
    "            'LC08',\n",
    "            'LC09',\n",
    "           'LE07',\n",
    "           'LT05'\n",
    "           ]\n",
    "cloudCoverFilter = {'min' : 0, 'max' : 75}\n",
    "fileType = 'band'\n",
    "# user input ends\n",
    "################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4354771b-9406-4efe-a093-cefc2d22e94c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read spatial extent from a csv\n",
    "aoi_df = pd.read_csv('tile_aoi.csv')\n",
    "aoi_df = aoi_df[aoi_df['tile']==tile_number]\n",
    "# corner coordinate\n",
    "llx = aoi_df['llx'].to_list()[0]\n",
    "lly = aoi_df['lly'].to_list()[0]\n",
    "urx = aoi_df['urx'].to_list()[0]\n",
    "ury = aoi_df['ury'].to_list()[0]\n",
    "# create a spatial filter\n",
    "spatialFilter = {\n",
    "    \"filterType\": \"mbr\",\n",
    "    \"lowerLeft\": {\n",
    "        \"latitude\": lly,\n",
    "        \"longitude\": llx\n",
    "    },\n",
    "    \"upperRight\": {\n",
    "        \"latitude\": ury ,\n",
    "        \"longitude\": urx\n",
    "    }\n",
    "}\n",
    "\n",
    "pprint.pprint(spatialFilter)\n",
    "# delete df\n",
    "del aoi_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c48c8ab-1c9f-4672-ae30-88621cfa1abb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sendRequest(url, data, apiKey=None, exitIfNoResponse=True):\n",
    "    \"\"\"\n",
    "    Send a request to an M2M endpoint and return the parsed JSON response.\n",
    "\n",
    "    Parameters:\n",
    "    url (str): The URL of the M2M endpoint\n",
    "    data (dict): The payload to be sent with the request\n",
    "    apiKey (str): Optional API key for authentication\n",
    "    exitIfNoResponse (bool): Whether to stop execution on failure\n",
    "\n",
    "    Returns:\n",
    "    dict or None: Parsed JSON response or None on error\n",
    "    \"\"\"\n",
    "    json_data = json.dumps(data)\n",
    "\n",
    "    try:\n",
    "        if apiKey is None:\n",
    "            response = requests.post(url, json_data)\n",
    "        else:\n",
    "            headers = {'X-Auth-Token': apiKey}\n",
    "            response = requests.post(url, json_data, headers=headers)\n",
    "\n",
    "        httpStatusCode = response.status_code\n",
    "        if response is None:\n",
    "            print(\"No output from service.\")\n",
    "            return None\n",
    "\n",
    "        output = json.loads(response.text)\n",
    "\n",
    "        if output.get('errorCode') is not None:\n",
    "            print(f\"{output['errorCode']} - {output['errorMessage']}\")\n",
    "            return None\n",
    "\n",
    "        if httpStatusCode == 404:\n",
    "            print(\"404 Not Found\")\n",
    "            return None\n",
    "        elif httpStatusCode == 401:\n",
    "            print(\"401 Unauthorized\")\n",
    "            return None\n",
    "        elif httpStatusCode == 400:\n",
    "            print(\"400 Bad Request\")\n",
    "            return None\n",
    "\n",
    "        return output.get('data', None)\n",
    "\n",
    "    except Exception as e:\n",
    "        raise RuntimeError(f\"Request failed: {e}\")\n",
    "    finally:\n",
    "        if 'response' in locals():\n",
    "            response.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18bba461-d210-4adc-8677-38e84462e53e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def downloadFile(url):\n",
    "    sema.acquire()\n",
    "    try:\n",
    "        with requests.get(url, stream=True, timeout=120) as response:\n",
    "            response.raise_for_status()\n",
    "\n",
    "            # Parse filename from content-disposition\n",
    "            disposition = response.headers.get('content-disposition', '')\n",
    "            matches = re.findall('filename=\"?([^\"]+)\"?', disposition)\n",
    "            filename = matches[0] if matches else f\"file_{int(time.time())}.dat\"\n",
    "\n",
    "            filepath = os.path.join(data_dir, filename)\n",
    "\n",
    "            # Stream the file to disk to save memory\n",
    "            with open(filepath, 'wb') as f:\n",
    "                for chunk in response.iter_content(chunk_size=8192):\n",
    "                    if chunk:  # filter out keep-alive chunks\n",
    "                        f.write(chunk)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"\\nFailed to download from {url}. Error: {e}\")\n",
    "        # Reattempt download (but avoid unbounded recursion)\n",
    "        try:\n",
    "            runDownload(threads, url)\n",
    "        except Exception as retry_error:\n",
    "            print(f\"Retry also failed: {retry_error}\")\n",
    "    finally:\n",
    "        sema.release()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1fa0d10-723a-4312-82a3-3008d7fbb27a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# downloading function\n",
    "def runDownload(threads, url):\n",
    "    def downloadWithLimit(url):\n",
    "        with sema:  # enforce concurrency limit\n",
    "            downloadFile(url)\n",
    "\n",
    "    thread = threading.Thread(target=downloadWithLimit, args=(url,))\n",
    "    threads.append(thread)\n",
    "    thread.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd696fab-1401-44a6-92a1-670f9a7c811f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define required directories\n",
    "data_dir = 'data_ls'\n",
    "utils_dir = 'utils'\n",
    "dirs = [data_dir, utils_dir]\n",
    "\n",
    "# Create directories if they don't exist\n",
    "for d in dirs:\n",
    "    try:\n",
    "        os.makedirs(d, exist_ok=True)\n",
    "        print(f\"Directory '{d}' is ready.\")  # Unified message\n",
    "    except OSError as e:\n",
    "        print(f\"[Error] Could not create '{d}': {e}\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fa548c5-239f-49e5-97b5-3ed6e57a4f9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the maximum number of concurrent download threads\n",
    "MAX_THREADS = 50\n",
    "sema = threading.Semaphore(value=MAX_THREADS)\n",
    "\n",
    "# Generate a timestamped label for logging or output folders\n",
    "label = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "# Thread list to keep track of active downloads\n",
    "threads = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8acd378a-a275-4a4d-9a15-85fbf659a4e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prompt_ERS_login(serviceURL):\n",
    "    print(\"Logging in...\\n\")\n",
    "\n",
    "    p = ['Enter EROS Registration System (ERS) Username: ', 'Enter ERS Account Token: ']\n",
    "\n",
    "    # Use requests.post() to make the login request\n",
    "    response = requests.post(f\"{serviceURL}login-token\",\n",
    "                             json={'username': '*****************************',\n",
    "                                    'token':'********************************************'})\n",
    "\n",
    "    if response.status_code == 200:  # Check for successful response\n",
    "        apiKey = response.json()['data']\n",
    "        print('\\nLogin Successful, API Key Received!')\n",
    "        headers = {'X-Auth-Token': apiKey}\n",
    "        return apiKey\n",
    "    else:\n",
    "        print(\"\\nLogin was unsuccessful, please try again or create an account at: https://ers.cr.usgs.gov/register.\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27477e5b-10e4-4823-8939-9afdef4b6160",
   "metadata": {},
   "outputs": [],
   "source": [
    "serviceUrl = \"https://m2m.cr.usgs.gov/api/api/json/stable/\"\n",
    "apiKey = prompt_ERS_login(serviceUrl) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24c28b5e-a7ce-41d7-bbd1-ef5b46fd69d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# count time for image loading\n",
    "import time\n",
    "start_time = time.time()\n",
    "\n",
    "# single sensor overide\n",
    "sensors = [\n",
    "    'LT05',\n",
    "    'LE07',\n",
    "    'LC08',\n",
    "    'LC09'\n",
    "    ] \n",
    "# walk over each sensor \n",
    "for sensor in sensors:\n",
    "    print(sensor)\n",
    "    if sensor == 'LT05':\n",
    "        bandNames = {'SR_B1', 'SR_B2', 'SR_B3', 'SR_B4', 'SR_B5', 'SR_B7', 'QA_PIXEL', 'ST_B6'}\n",
    "        temporal_coverage = {'start' : '1984-03-01', 'end' : '2012-05-05'}# mission do not have images after 2011 november\n",
    "        #temporal_coverage = {'start' : '1994-01-01', 'end' : '2012-05-05'}\n",
    "        #print(bandNames)\n",
    "    elif sensor == 'LE07':\n",
    "        bandNames = {'SR_B1', 'SR_B2', 'SR_B3', 'SR_B4', 'SR_B5', 'SR_B7', 'QA_PIXEL', 'ST_B6'}\n",
    "        temporal_coverage = {'start' : '1999-01-01', 'end' : '2022-04-06'} # mission ended 2022-04-06\n",
    "        #temporal_coverage = {'start' : '2002-01-01', 'end' : '2003-01-01'}\n",
    "        #print(bandNames)\n",
    "    elif sensor == 'LC09':\n",
    "        bandNames = {'SR_B2', 'SR_B3', 'SR_B4', 'SR_B5', 'SR_B6', 'SR_B7', 'QA_PIXEL', 'ST_B10'}\n",
    "        temporal_coverage = {'start' : '2021-10-31', 'end' : '2024-12-31'}\n",
    "        #temporal_coverage = {'start' : '2024-01-01', 'end' : '2024-12-31'}\n",
    "    else:\n",
    "        bandNames = {'SR_B2', 'SR_B3', 'SR_B4', 'SR_B5', 'SR_B6', 'SR_B7', 'QA_PIXEL', 'ST_B10'}\n",
    "        temporal_coverage = {'start' : '2013-01-01', 'end' : '2024-12-31'}\n",
    "        #temporal_coverage = {'start' : '2014-01-01', 'end' : '2024-10-01'}\n",
    "        #print(bandNames)\n",
    "\n",
    "    # temporal filter: list temporal filter  dicts\n",
    "    # just devide the large temporal range to anual cycles to avoid bad response from api\n",
    "    dates = pd.date_range(start=temporal_coverage['start'], \n",
    "                      end=temporal_coverage['end'], \n",
    "                      freq='YS') \n",
    "    # add final date manually\n",
    "    dates = dates.append(pd.to_datetime([temporal_coverage['end']]))\n",
    "    # create a list of dict using loop\n",
    "    temp_list = [{'start': str(dates[i].date()), 'end': str(dates[i+1].date())} for i in range(len(dates)-1)]\n",
    "    # start downloading for each: after this line\n",
    "    for temporalFilter in temp_list:\n",
    "            # search payload\n",
    "            search_payload = {\n",
    "            'datasetName' : datasetName,\n",
    "            'sceneFilter' : {\n",
    "                'spatialFilter' : spatialFilter,\n",
    "                'acquisitionFilter' : temporalFilter,\n",
    "                'cloudCoverFilter' : cloudCoverFilter}\n",
    "                }\n",
    "            # scene search\n",
    "            scenes = sendRequest(serviceUrl + \"scene-search\", search_payload, apiKey)\n",
    "            # idfeild to grab\n",
    "            idField = 'entityId'\n",
    "            # entity id list\n",
    "            entityIds = []\n",
    "            # take entid if bulk true\n",
    "            for result in scenes['results']:\n",
    "                # Add this scene to the list I would like to download if bulk is available\n",
    "                if result['options']['bulk'] == True:\n",
    "                    entityIds.append(result[idField])\n",
    "            # filter for sensor\n",
    "            # select only a single sensor: so the only one sensor goes to next list ids\n",
    "            entityIds = [item for item in entityIds if item[:4] == sensor]\n",
    "            if len(entityIds) == 0: continue\n",
    "            listId = f\"temp_{datasetName}_list\" # customized list id\n",
    "            # scn list payload: mesg from me to api\n",
    "            scn_list_add_payload = {\n",
    "                \"listId\": listId,\n",
    "                'idField' : idField,\n",
    "                \"entityIds\": entityIds,\n",
    "                \"datasetName\": datasetName\n",
    "                }\n",
    "            # clean old requests: otherwise it mixed up new request with old\n",
    "            sendRequest(serviceUrl + \"scene-list-remove\", {\"listId\": listId}, apiKey) \n",
    "            # number of image count\n",
    "            count = sendRequest(serviceUrl + \"scene-list-add\", scn_list_add_payload, apiKey)\n",
    "            print(f'number of images to download {count}')\n",
    "            # add download code; rename old folder\n",
    "            sendRequest(serviceUrl + \"scene-list-get\", {'listId' : scn_list_add_payload['listId']}, apiKey)\n",
    "            # \n",
    "            download_opt_payload = {\n",
    "                        \"listId\": listId,\n",
    "                        \"datasetName\": datasetName\n",
    "                        }\n",
    "            #if fileType == 'band_group':\n",
    "            #     download_opt_payload['includeSecondaryFileGroups'] = True\n",
    "            products = sendRequest(serviceUrl + \"download-options\", download_opt_payload, apiKey)\n",
    "            filegroups = sendRequest(serviceUrl + \"dataset-file-groups\", {'datasetName' : datasetName}, apiKey)\n",
    "\n",
    "            # file group id\n",
    "            fileGroupIds = {\"ls_c2ard_sr\"} # can change this\n",
    "\n",
    "            # Select products\n",
    "            print(\"Selecting products...\")\n",
    "            downloads = []\n",
    "            if fileType == 'bundle':\n",
    "                # Select bundle files\n",
    "                print(\"    Selecting bundle files...\")\n",
    "                for product in products:        \n",
    "                    if product[\"bulkAvailable\"] and product['downloadSystem'] != 'folder':               \n",
    "                        downloads.append({\"entityId\":product[\"entityId\"], \"productId\":product[\"id\"]})\n",
    "\n",
    "\n",
    "            elif fileType == 'band':\n",
    "                # Select band files\n",
    "                print(\"    Selecting band files...\")\n",
    "                for product in products:  \n",
    "                    if product[\"secondaryDownloads\"] is not None and len(product[\"secondaryDownloads\"]) > 0:\n",
    "                        for secondaryDownload in product[\"secondaryDownloads\"]:\n",
    "                            for bandName in bandNames:\n",
    "                                if secondaryDownload[\"bulkAvailable\"] and bandName in secondaryDownload['displayId']:\n",
    "                                    downloads.append({\"entityId\":secondaryDownload[\"entityId\"], \"productId\":secondaryDownload[\"id\"]})\n",
    "\n",
    "\n",
    "            elif fileType == 'band_group':        \n",
    "                # Get secondary dataset ID and file group IDs with the scenes\n",
    "                print(\"    Checking for scene band groups and get secondary dataset ID and file group IDs with the scenes...\")\n",
    "                sceneFileGroups = []\n",
    "                entityIds = []\n",
    "                datasetId = None\n",
    "                for product in products:  \n",
    "                    if product[\"secondaryDownloads\"] is not None and len(product[\"secondaryDownloads\"]) > 0:\n",
    "                        for secondaryDownload in product[\"secondaryDownloads\"]:\n",
    "                            if secondaryDownload[\"bulkAvailable\"] and secondaryDownload[\"fileGroups\"] is not None:\n",
    "                                if datasetId == None:\n",
    "                                    datasetId = secondaryDownload['datasetId']\n",
    "                                for fg in secondaryDownload[\"fileGroups\"]:                            \n",
    "                                    if fg not in sceneFileGroups:\n",
    "                                        sceneFileGroups.append(fg)\n",
    "                                    if secondaryDownload['entityId'] not in entityIds:\n",
    "                                        entityIds.append(secondaryDownload['entityId'])\n",
    "\n",
    "                # Send dataset request to get the secondary dataset name by the dataset ID\n",
    "                data_req_payload = {\n",
    "                    \"datasetId\": datasetId,\n",
    "                }\n",
    "                results = sendRequest(serviceUrl + \"dataset\", data_req_payload, apiKey)\n",
    "                secondaryDatasetName = results['datasetAlias']\n",
    "\n",
    "                # Add secondary scenes to a list\n",
    "                secondaryListId = f\"temp_{datasetName}_scecondary_list\" # customized list id\n",
    "                sec_scn_add_payload = {\n",
    "                    \"listId\": secondaryListId,\n",
    "                    \"entityIds\": entityIds,\n",
    "                    \"datasetName\": secondaryDatasetName\n",
    "                }\n",
    "\n",
    "                print(\"    Adding secondary scenes to list...\")\n",
    "                count = sendRequest(serviceUrl + \"scene-list-add\", sec_scn_add_payload, apiKey)    \n",
    "                print(\"    Added\", count, \"secondary scenes\\n\")\n",
    "\n",
    "                # Compare the provided file groups Ids with the scenes' file groups IDs\n",
    "                if fileGroupIds:\n",
    "                    fileGroups = []\n",
    "                    for fg in fileGroupIds:\n",
    "                        fg = fg.strip() \n",
    "                        if fg in sceneFileGroups:\n",
    "                            fileGroups.append(fg)\n",
    "                else:\n",
    "                    fileGroups = sceneFileGroups\n",
    "            else:\n",
    "                # Select all available files\n",
    "                for product in products:        \n",
    "                    if product[\"bulkAvailable\"]:\n",
    "                        if product['downloadSystem'] != 'folder':            \n",
    "                            downloads.append({\"entityId\":product[\"entityId\"], \"productId\":product[\"id\"]})\n",
    "                        if product[\"secondaryDownloads\"] is not None and len(product[\"secondaryDownloads\"]) > 0:\n",
    "                            for secondaryDownload in product[\"secondaryDownloads\"]:\n",
    "                                if secondaryDownload[\"bulkAvailable\"]:\n",
    "                                    downloads.append({\"entityId\":secondaryDownload[\"entityId\"], \"productId\":secondaryDownload[\"id\"]})\n",
    "\n",
    "                        \n",
    "            \n",
    "\n",
    "\n",
    "            # sending download request\n",
    "            if fileType != 'band_group':\n",
    "                download_req2_payload = {\n",
    "                    \"downloads\": downloads,\n",
    "                    \"label\": label\n",
    "                }\n",
    "            else:\n",
    "                if len(fileGroups) > 0:\n",
    "                    download_req2_payload = {\n",
    "                        \"dataGroups\": [\n",
    "                            {\n",
    "                                \"fileGroups\": fileGroups,\n",
    "                                \"datasetName\": secondaryDatasetName,\n",
    "                                \"listId\": secondaryListId\n",
    "                            }\n",
    "                        ],\n",
    "                        \"label\": label\n",
    "                    }\n",
    "                else:\n",
    "                    print('No file groups found')\n",
    "                    sys.exit()\n",
    "\n",
    "            print(f\"Sending download request ...\")\n",
    "            download_request_results = sendRequest(serviceUrl + \"download-request\", download_req2_payload, apiKey)\n",
    "            print(f\"Done sending download request\") \n",
    "\n",
    "            if len(download_request_results['newRecords']) == 0 and len(download_request_results['duplicateProducts']) == 0:\n",
    "                print('No records returned, please update your scenes or scene-search filter')\n",
    "                sys.exit()\n",
    "\n",
    "            # Attempt the download URLs \n",
    "            for result in download_request_results['availableDownloads']:\n",
    "                #print(f\"Get download url: {result['url']}\\n\" )\n",
    "                runDownload(threads, result['url'])\n",
    "                \n",
    "            preparingDownloadCount = len(download_request_results['preparingDownloads'])\n",
    "            preparingDownloadIds = []\n",
    "            if preparingDownloadCount > 0:\n",
    "                for result in download_request_results['preparingDownloads']:  \n",
    "                    preparingDownloadIds.append(result['downloadId'])\n",
    "\n",
    "                download_ret_payload = {\"label\" : label}                \n",
    "                # Retrieve download URLs\n",
    "                print(\"Retrieving download urls...\\n\")\n",
    "                download_retrieve_results = sendRequest(serviceUrl + \"download-retrieve\", download_ret_payload, apiKey, False)\n",
    "                if download_retrieve_results != False:\n",
    "                    print(f\"    Retrieved: \\n\" )\n",
    "                    for result in download_retrieve_results['available']:\n",
    "                        if result['downloadId'] in preparingDownloadIds:\n",
    "                            preparingDownloadIds.remove(result['downloadId'])\n",
    "                            runDownload(threads, result['url'])\n",
    "                            print(f\"       {result['url']}\\n\" )\n",
    "                        \n",
    "                    for result in download_retrieve_results['requested']:   \n",
    "                        if result['downloadId'] in preparingDownloadIds:\n",
    "                            preparingDownloadIds.remove(result['downloadId'])\n",
    "                            runDownload(threads, result['url'])\n",
    "                            print(f\"       {result['url']}\\n\" )\n",
    "                \n",
    "                # Didn't get all download URLs, retrieve again after 30 seconds\n",
    "                while len(preparingDownloadIds) > 0: \n",
    "                    print(f\"{len(preparingDownloadIds)} downloads are not available yet. Waiting for 30s to retrieve again\\n\")\n",
    "                    time.sleep(30)\n",
    "                    download_retrieve_results = sendRequest(serviceUrl + \"download-retrieve\", download_ret_payload, apiKey, False)\n",
    "                    if download_retrieve_results != False:\n",
    "                        for result in download_retrieve_results['available']:                            \n",
    "                            if result['downloadId'] in preparingDownloadIds:\n",
    "                                preparingDownloadIds.remove(result['downloadId'])\n",
    "                                #print(f\"    Get download url: {result['url']}\\n\" )\n",
    "                                runDownload(threads, result['url'])\n",
    "                                \n",
    "            print(\"\\nDownloading files... Please do not close the program\\n\")\n",
    "            for thread in threads:\n",
    "                thread.join() \n",
    "\n",
    "            # create a log file\n",
    "            with open(tile_number+\"_logFile.txt\", \"w\") as file:\n",
    "                # write info into logfile\n",
    "                start_year = temporalFilter['start'][:4]\n",
    "                text = f\"Images available for year {start_year} is {count}\\n\"\n",
    "                file.write(sensor + \" \")\n",
    "                file.write(text)\n",
    "\n",
    "\n",
    "# rename the folder to the tile ID\n",
    "os.rename('data_ls', tile_number)\n",
    "\n",
    "# logging out from the system\n",
    "endpoint = \"logout\"  \n",
    "if sendRequest(serviceUrl + endpoint, None, apiKey) == None:        \n",
    "    print(\"\\nLogged Out\\n\")\n",
    "else:\n",
    "    print(\"\\nLogout Failed\\n\")\n",
    "\n",
    "# count time taken for the process\n",
    "end_time = time.time()\n",
    "elapsed_time = end_time - start_time\n",
    "print(f\"\\nTotal time taken: {elapsed_time / 60:.2f} minutes ({elapsed_time:.2f} seconds)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6150695b-8e8c-4049-ace8-34a803d7a0f0",
   "metadata": {},
   "source": [
    "Execute following cell if need to clean up the payload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a98d1067-044b-4bcb-bc2a-37778b0b0cc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "remove_scnlst_payload = {\n",
    "    \"listId\": listId\n",
    "}\n",
    "sendRequest(serviceUrl + \"scene-list-remove\", remove_scnlst_payload, apiKey)\n",
    "\n",
    "if fileType == 'band_group':    \n",
    "    # Remove the secondary scene list\n",
    "    remove_scnlst2_payload = {\n",
    "        \"listId\": secondaryListId\n",
    "    }\n",
    "    sendRequest(serviceUrl + \"scene-list-remove\", remove_scnlst2_payload, apiKey)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c628ab5c-72d4-47de-be57-d34d358a49ce",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
