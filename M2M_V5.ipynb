{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import requests\n",
    "import threading\n",
    "import time\n",
    "import datetime\n",
    "import pandas as pd\n",
    "from getpass import getpass\n",
    "from pprint import pprint\n",
    "\n",
    "# Suppress warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# User inputs\n",
    "TILE_NUMBER = 'h3v9'\n",
    "DATASET_NAME = 'landsat_ard_tile_c2'\n",
    "SENSORS = [\n",
    "    'LT05',  # Only processing LT05 for now\n",
    "]\n",
    "CLOUD_COVER_FILTER = {'min': 0, 'max': 75}\n",
    "FILE_TYPE = 'band'\n",
    "DATA_DIR = 'data'\n",
    "MAX_THREADS = 5\n",
    "\n",
    "# Load AOI information\n",
    "aoi_df = pd.read_csv('tile_aoi.csv')\n",
    "aoi_df = aoi_df[aoi_df['tile'] == TILE_NUMBER].iloc[0]\n",
    "spatial_filter = {\n",
    "    \"filterType\": \"mbr\",\n",
    "    \"lowerLeft\": {\"latitude\": aoi_df['lly'], \"longitude\": aoi_df['llx']},\n",
    "    \"upperRight\": {\"latitude\": aoi_df['ury'], \"longitude\": aoi_df['urx']}\n",
    "}\n",
    "\n",
    "pprint(spatial_filter)\n",
    "\n",
    "# Helper function to create directories\n",
    "def create_directory(path):\n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)\n",
    "        print(f\"Directory '{path}' created successfully.\")\n",
    "    else:\n",
    "        print(f\"Directory '{path}' already exists.\")\n",
    "\n",
    "# HTTP request sender\n",
    "def send_request(url, payload, api_key=None):\n",
    "    headers = {'X-Auth-Token': api_key} if api_key else {}\n",
    "    response = requests.post(url, json=payload, headers=headers)\n",
    "    response.raise_for_status()\n",
    "    result = response.json()\n",
    "    if result.get('errorCode'):\n",
    "        raise Exception(f\"{result['errorCode']}: {result['errorMessage']}\")\n",
    "    return result['data']\n",
    "\n",
    "# Function to remove previous scene lists\n",
    "def remove_scene_list(service_url, list_id, api_key):\n",
    "    try:\n",
    "        payload = {\"listId\": list_id}\n",
    "        send_request(f\"{service_url}scene-list-remove\", payload, api_key)\n",
    "        print(f\"Successfully removed list: {list_id}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to remove list {list_id}: {e}\")\n",
    "\n",
    "# Download function with retry\n",
    "def download_file(url):\n",
    "    sema.acquire()\n",
    "    try:\n",
    "        response = requests.get(url, stream=True)\n",
    "        filename = response.headers['content-disposition'].split('filename=')[-1].strip(\"\\\"\")\n",
    "        print(f\"Downloading: {filename}...\")\n",
    "        with open(os.path.join(DATA_DIR, filename), 'wb') as file:\n",
    "            file.write(response.content)\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to download from {url}. Retrying...\")\n",
    "        run_download(url)\n",
    "    finally:\n",
    "        sema.release()\n",
    "\n",
    "# Threaded download\n",
    "def run_download(url):\n",
    "    thread = threading.Thread(target=download_file, args=(url,))\n",
    "    threads.append(thread)\n",
    "    thread.start()\n",
    "\n",
    "# Login to USGS M2M API\n",
    "def login_to_usgs(service_url):\n",
    "    username = input(\"Enter USGS Username: \")\n",
    "    token = getpass(\"Enter USGS API Token: \")\n",
    "    response = requests.post(f\"{service_url}login-token\", json={\"username\": username, \"token\": token})\n",
    "    response.raise_for_status()\n",
    "    return response.json()['data']\n",
    "\n",
    "# Main execution\n",
    "def main():\n",
    "    service_url = \"https://m2m.cr.usgs.gov/api/api/json/stable/\"\n",
    "    api_key = login_to_usgs(service_url)\n",
    "\n",
    "    create_directory(DATA_DIR)\n",
    "\n",
    "    for sensor in SENSORS:\n",
    "        print(f\"Processing sensor: {sensor}\")\n",
    "        temporal_ranges = {\n",
    "            'LT05': {'start': '2006-01-01', 'end': '2012-05-05'},\n",
    "        }\n",
    "        temporal_coverage = temporal_ranges[sensor]\n",
    "        date_ranges = pd.date_range(start=temporal_coverage['start'], end=temporal_coverage['end'], freq='YS')\n",
    "        date_ranges = date_ranges.append(pd.to_datetime([temporal_coverage['end']]))\n",
    "\n",
    "        for i in range(len(date_ranges) - 1):\n",
    "            temporal_filter = {'start': str(date_ranges[i].date()), 'end': str(date_ranges[i + 1].date())}\n",
    "            payload = {\n",
    "                'datasetName': DATASET_NAME,\n",
    "                'sceneFilter': {\n",
    "                    'spatialFilter': spatial_filter,\n",
    "                    'acquisitionFilter': temporal_filter,\n",
    "                    'cloudCoverFilter': CLOUD_COVER_FILTER\n",
    "                }\n",
    "            }\n",
    "            scenes = send_request(f\"{service_url}scene-search\", payload, api_key)\n",
    "\n",
    "            # Debugging logs\n",
    "            print(f\"Payload for {sensor}: {payload}\")\n",
    "            pprint(scenes)\n",
    "\n",
    "            entity_ids = [s['entityId'] for s in scenes['results'] if s['options']['bulk']]\n",
    "            print(f\"Entity IDs for {sensor}: {entity_ids}\")\n",
    "\n",
    "            if not entity_ids:\n",
    "                continue\n",
    "\n",
    "            list_id = f\"{sensor}_scene_list\"\n",
    "            remove_scene_list(service_url, list_id, api_key)  # Clean up previous lists\n",
    "            list_payload = {\"listId\": list_id, \"idField\": \"entityId\", \"entityIds\": entity_ids, \"datasetName\": DATASET_NAME}\n",
    "            send_request(f\"{service_url}scene-list-add\", list_payload, api_key)\n",
    "\n",
    "            download_payload = {\"listId\": list_id, \"datasetName\": DATASET_NAME}\n",
    "            products = send_request(f\"{service_url}download-options\", download_payload, api_key)\n",
    "            downloads = [{\"entityId\": p[\"entityId\"], \"productId\": p[\"id\"]} for p in products if p['bulkAvailable']]\n",
    "            download_request = {\"downloads\": downloads, \"label\": datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")}\n",
    "            download_results = send_request(f\"{service_url}download-request\", download_request, api_key)\n",
    "\n",
    "            for dl in download_results['availableDownloads']:\n",
    "                run_download(dl['url'])\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    sema = threading.Semaphore(MAX_THREADS)\n",
    "    threads = []\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "geospatial",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
